{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 파노라마(panorama) 영상 만들기\n",
    "## 1) 과정\n",
    "### ① 지역 특징 추출\n",
    "- SIFT(Scale Invariant Feature Transform) 검출기를 사용하여 각각의 영상에서 지역 특징점 검출\n",
    "\n",
    "### ② 대응점 매칭  \n",
    "- ①에서 찾은 지역 특징점 간 매칭 수행\n",
    "- 영상 1의 i번째 특징 $a_i$와 대응되는 영상 2의 j번째 특징 $b_j$를 찾은 후, 매칭 쌍으로 저장함  \n",
    "- 매칭 전략    \n",
    "  - 1) 고정 임계값 사용  \n",
    "    - $d(a_i,b_j) < T$\n",
    "  - 2) 최근접 특징 벡터 탐색\n",
    "  - 3) 최근접 거리 비율 **(실습에서는 이 매칭 방식 사용)**\n",
    "    - $\\frac{d(a_i,b_j)}{d(a_i,b_k)} < T$  \n",
    "    - $a_i$와 거리가 가까운 상위 2개의 대응점 $b_j$와 $b_k$을 찾은 후, 미리 설정한 거리 비율을 만족하는 특징만 저장함 (즉, 확실한 매칭쌍만 남김)\n",
    "\n",
    "### ③ 변환 행렬($H$) 추정 \n",
    "- 영상 1과 영상 2 사이의 다수의 매칭쌍으로부터 **RANSAC(RANdom SAmple Consencus)**을 이용하여 변환 행렬 추정  \n",
    "- RANSAC은 이상점(outlier)이 포함된 데이터셋에서 어떠한 모델을 예측할 때 효과적인 방법임\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "    H = \\begin{bmatrix}  \n",
    "        h_{00} & h_{01} & h_{02} \\\\  \n",
    "        h_{10} & h_{11} & h_{12} \\\\  \n",
    "        h_{20} & h_{21} & h_{22} \\\\  \n",
    "        \\end{bmatrix}  \n",
    "\\end{equation}  \n",
    "$$  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "        x' \\\\\n",
    "        y' \\\\\n",
    "        1\n",
    "    \\end{bmatrix} \n",
    "    = H\n",
    "    \\begin{bmatrix}\n",
    "        x \\\\\n",
    "        y \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        h_{00} & h_{01} & h_{02} \\\\\n",
    "        h_{10} & h_{11} & h_{12} \\\\\n",
    "        h_{20} & h_{21} & h_{22}\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        x \\\\\n",
    "        y \\\\\n",
    "        1\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "### ④ 원근 변환(perspective transform ≒ homography) 적용\n",
    "- 추정된 변환 행렬 $H$를 이용하여 영상 2에 원근 변환 적용\n",
    "\n",
    "### ⑤ 두 영상 이어붙이기\n",
    "- 파노라마 결과 영상 = 영상 1 + 변환된 영상 2\n",
    "\n",
    "#  2. Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## 테스트할 이미지\n",
    "img_1 = cv2.imread('../left.jpg')\n",
    "img_1 = cv2.resize(img_1, None, fx=0.5, fy=0.5)\n",
    "gray_1 = cv2.cvtColor(img_1,cv2.COLOR_BGR2GRAY)\n",
    "img_2 = cv2.imread('../right.jpg')\n",
    "img_2 = cv2.resize(img_2, None, fx=0.5, fy=0.5)\n",
    "gray_2 = cv2.cvtColor(img_2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "## 이미지 확인\n",
    "cv2.imshow('image 1', img_1)\n",
    "cv2.imshow('image 2', img_2)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of original matching pairs: 1254\n",
      "# of good matching pairs: 243\n"
     ]
    }
   ],
   "source": [
    "## 1) 각각의 영상에서 SIFT 특징 추출\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "kp_1, des_1 = sift.detectAndCompute(gray_1, None)\n",
    "kp_2, des_2 = sift.detectAndCompute(gray_2, None)\n",
    "\n",
    "## 2-1) 두 영상의 지역 특징 간 거리 계산\n",
    "## https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_descriptor_matchers.html#bfmatcher\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(queryDescriptors = des_1,\n",
    "                      trainDescriptors = des_2,\n",
    "                      k=2)   # queryDescriptors의 각 특징점마다 거리가 가장 가까운 상위 k개의 trainDescriptors 특징점을 찾아줌\n",
    "\n",
    "## 2-2) '최근접 거리 비율' 매칭 전략('ratio testing')을 사용하여 매칭쌍 생성\n",
    "ratio = 0.7  # 보통 0.7 ~ 0.8 값 사용\n",
    "good = []    # 좋은 대응점 쌍만 따로 저장할 리스트\n",
    "for m,n in matches:   # m: 가장 가까운 매칭쌍 / n: 2번째로 가까운 매칭쌍\n",
    "    if m.distance < n.distance * ratio:\n",
    "        good.append(m)\n",
    "        \n",
    "print(\"# of original matching pairs: %d\" % len(des_1))\n",
    "print(\"# of good matching pairs: %d\" % len(good))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## optional) SIFT 특징점 매칭 결과 확인하기\n",
    "matched = cv2.drawMatches(img1 = img_1, \n",
    "                          keypoints1 = kp_1, \n",
    "                          img2 = img_2, \n",
    "                          keypoints2 = kp_2, \n",
    "                          matches1to2 = good[:20],   # 첫 20개 매칭쌍만 시각화\n",
    "                          outImg = None, \n",
    "                          flags = 2)\n",
    "\n",
    "cv2.imshow('matching result', matched)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H.shape: (3, 3)\n",
      "[[ 7.69772440e-01  4.77361221e-02  2.21585348e+02]\n",
      " [-1.35461911e-01  9.21431020e-01  3.67724987e+01]\n",
      " [-4.24829039e-04 -4.68290400e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "## 3) RANSAC을 이용한 변환 행렬 H 추정\n",
    "## 참고. cv2.getPerspectiveTransform()와 cv2.findHomography()의 차이\n",
    "##   cv2.getPerspectiveTransform() - 4개의 대응점 쌍을 입력하면 변환 행렬을 반환해줌\n",
    "##   cv2.findHomography() - 4개 이상의 대응점 쌍을 입력하면 가장 대응점들을 가장 잘 만족하는 변환 행렬을 반환해줌\n",
    "## https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#findhomography\n",
    "if len(good) >= 4:   # 4개 이상의 대응점이 존재해야 원근 변환 행렬을 추정 가능\n",
    "    src_pts = np.float32([ kp_2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    dst_pts = np.float32([ kp_1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "    H, _ = cv2.findHomography(srcPoints = src_pts, \n",
    "                              dstPoints = dst_pts, \n",
    "                              method = cv2.RANSAC, \n",
    "                              ransacReprojThreshold = 5)\n",
    "    \n",
    "    print(\"H.shape: {}\".format(H.shape))\n",
    "    print(H)\n",
    "    \n",
    "else:\n",
    "    raise AssertionError(\"Can't find enough keypoints.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4) 원근 변환 적용\n",
    "## 변환 행렬 H를 사용하여 변환 수행\n",
    "## https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#warpperspective\n",
    "res = cv2.warpPerspective(src = img_2,\n",
    "                          M = H,\n",
    "                          dsize = (img_1.shape[1]+img_2.shape[1], img_1.shape[0]))\n",
    "\n",
    "# cv2.imshow('right image', img_r)\n",
    "# cv2.imshow('transformed right image', res)\n",
    "# cv2.imshow('left image', img_l)\n",
    "\n",
    "## 5) 두 영상 이어붙이기\n",
    "res[0:img_1.shape[0], 0:img_1.shape[1]] = img_1 # 왼쪽 영상을 변환된 오른쪽 영상에 오버랩함\n",
    "cv2.imshow('panorama', res)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras] *",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
